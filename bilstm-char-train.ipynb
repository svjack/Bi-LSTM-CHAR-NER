{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle as pkl\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict_path = \"args.json\"\n",
    "with open(params_dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    params_dict = json.load(f)\n",
    "from collections import namedtuple\n",
    "params_tuple = namedtuple(\"params_tuple\" ,params_dict.keys())\n",
    "for k, v in params_dict.items():\n",
    "    exec(\"params_tuple.{}={}\".format(k, \"'{}'\".format(v) if type(v) == type(\"\") else v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.params_tuple"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_file_first_rows(file_path, rows= 100):\n",
    "    line_list = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cnt = 0\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line or cnt >= rows:\n",
    "                break\n",
    "            line_list.append(\"{}\\n\".format(line.strip()))\n",
    "            cnt += 1\n",
    "    return \"\".join(line_list)\n",
    "\n",
    "def identify_sep(file_path):\n",
    "    first_lines_str = show_file_first_rows(file_path)\n",
    "    lines = first_lines_str.split(\"\\n\")\n",
    "    spliters = [\"\\t\", \" \"]\n",
    "    sep_cnt_dict = dict(map(lambda sep: (sep ,sum(map(lambda l: len(l.split(sep)), lines))), spliters))\n",
    "    return list(map(lambda t2: t2[0],sorted(sep_cnt_dict.items(), key = lambda t2: -1 * t2[-1])))[0]\n",
    "\n",
    "import sys, pickle, os, random\n",
    "def produce_tag2label(input_series):\n",
    "    input_series = pd.Series(input_series).dropna()\n",
    "    return dict(map(lambda t2: (t2[1], t2[0]) ,enumerate(input_series.unique().tolist())))\n",
    "\n",
    "def read_corpus_by_pd(corpus_path):\n",
    "    sep = identify_sep(corpus_path)\n",
    "    return pd.read_csv(corpus_path, header = None, delimiter=sep, skip_blank_lines=False)\n",
    "\n",
    "def retrieve_sep_nest_list(corpus_df):\n",
    "    #na_line_where = np.where(pd.isna(corpus_df.iloc[:, -1]))[0]\n",
    "    #na_line_where = np.where(pd.isna(corpus_df.iloc[:, 0]))[0]\n",
    "    na_line_where = np.where(corpus_df.apply(lambda x: np.any(pd.isna(x)), axis = 1))\n",
    "    assert type(na_line_where) == type((0,))\n",
    "    na_line_where = na_line_where[0]\n",
    "    #assert 0 not in na_line_where and (corpus_df.shape[0] - 1) not in na_line_where\n",
    "    assert 0 not in na_line_where\n",
    "    nest_sep_indice_list = []\n",
    "    for idx in range(corpus_df.shape[0]):\n",
    "        if not nest_sep_indice_list:\n",
    "            nest_sep_indice_list.append([idx])\n",
    "        else:\n",
    "            if idx not in na_line_where:\n",
    "                nest_sep_indice_list[-1].append(idx)\n",
    "            else:\n",
    "                nest_sep_indice_list.append([])\n",
    "    nest_sep_indice_list = list(filter(len, nest_sep_indice_list))\n",
    "    return nest_sep_indice_list\n",
    "\n",
    "def read_corpus_by_pd_nest_list(corpus_path):\n",
    "    #### output format [([...], [...]), ] or [([...], [...], [...], [...]), ]\n",
    "    corpus_df = read_corpus_by_pd(corpus_path)\n",
    "    assert corpus_df.shape[1] in (4, 2)\n",
    "    nest_sep_list = retrieve_sep_nest_list(corpus_df)\n",
    "    for sliced_df in map(lambda inner_list: corpus_df.iloc[inner_list, :], nest_sep_list):\n",
    "        ele = list(zip(*sliced_df.values.tolist()))\n",
    "        yield ele\n",
    "        \n",
    "def vocab_build(vocab_path, corpus_path, min_count):\n",
    "    \"\"\"\n",
    "\n",
    "    :param vocab_path:\n",
    "    :param corpus_path:\n",
    "    :param min_count:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #data = read_corpus(corpus_path)\n",
    "    data = list(read_corpus_by_pd_nest_list(corpus_path))\n",
    "    word2id = {}\n",
    "    #for sent_, tag_ in data:\n",
    "    for t in data:\n",
    "        if len(data[0]) == 2:\n",
    "            sent_, tag_ = t\n",
    "        elif len(data[0]) == 4:\n",
    "            sent_, _, _, tag_ = t\n",
    "        else:\n",
    "            1 / 0\n",
    "        for word in sent_:\n",
    "            if word.isdigit():\n",
    "                word = '<NUM>'\n",
    "            elif ('\\u0041' <= word <='\\u005a') or ('\\u0061' <= word <='\\u007a'):\n",
    "                #word = '<ENG>'\n",
    "                pass\n",
    "            if word not in word2id:\n",
    "                word2id[word] = [len(word2id)+1, 1]\n",
    "            else:\n",
    "                word2id[word][1] += 1\n",
    "    low_freq_words = []\n",
    "    for word, [word_id, word_freq] in word2id.items():\n",
    "        if word_freq < min_count and word != '<NUM>' and word != '<ENG>':\n",
    "            low_freq_words.append(word)\n",
    "    for word in low_freq_words:\n",
    "        del word2id[word]\n",
    "\n",
    "    new_id = 1\n",
    "    for word in word2id.keys():\n",
    "        word2id[word] = new_id\n",
    "        new_id += 1\n",
    "    word2id['<UNK>'] = new_id\n",
    "    word2id['<PAD>'] = 0\n",
    "\n",
    "    print(len(word2id))\n",
    "    with open(vocab_path, 'wb') as fw:\n",
    "        pickle.dump(word2id, fw)\n",
    "\n",
    "def sentence2id(sent, word2id):\n",
    "    \"\"\"\n",
    "\n",
    "    :param sent:\n",
    "    :param word2id:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sentence_id = []\n",
    "    for word in sent:\n",
    "        if word.isdigit():\n",
    "            word = '<NUM>'\n",
    "        elif ('\\u0041' <= word <= '\\u005a') or ('\\u0061' <= word <= '\\u007a'):\n",
    "            #word = '<ENG>'\n",
    "            pass\n",
    "        if word not in word2id:\n",
    "            word = '<UNK>'\n",
    "        sentence_id.append(word2id[word])\n",
    "    return sentence_id\n",
    "\n",
    "def words_as_char2id(word, char2id):\n",
    "    word_id = []\n",
    "    for char in word:\n",
    "        if char not in char2id:\n",
    "            char = '<UNK>'\n",
    "        word_id.append(char2id[char])\n",
    "    return word_id\n",
    "\n",
    "def read_dictionary(vocab_path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param vocab_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    vocab_path = os.path.join(vocab_path)\n",
    "    with open(vocab_path, 'rb') as fr:\n",
    "        word2id = pickle.load(fr)\n",
    "    print('vocab_size:', len(word2id))\n",
    "    return word2id\n",
    "\n",
    "\n",
    "def random_embedding(vocab, embedding_dim):\n",
    "    \"\"\"\n",
    "\n",
    "    :param vocab:\n",
    "    :param embedding_dim:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    embedding_mat = np.random.uniform(-0.25, 0.25, (len(vocab), embedding_dim))\n",
    "    embedding_mat = np.float32(embedding_mat)\n",
    "    return embedding_mat\n",
    "\n",
    "def pad_sequences(sequences, pad_mark=0):\n",
    "    \"\"\"\n",
    "\n",
    "    :param sequences:\n",
    "    :param pad_mark:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    max_len = max(map(lambda x : len(x), sequences))\n",
    "    seq_list, seq_len_list = [], []\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_len] + [pad_mark] * max(max_len - len(seq), 0)\n",
    "        seq_list.append(seq_)\n",
    "        seq_len_list.append(min(len(seq), max_len))\n",
    "    return seq_list, seq_len_list\n",
    "\n",
    "def pad_char_sequences(sequences, char2id):\n",
    "    \"\"\"\n",
    "\n",
    "    :param sequences:\n",
    "    :param pad_mark:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #### sequences [batch, seq_len, word_len,]\n",
    "    assert \"<PAD>\" in char2id\n",
    "    pad_mark = char2id[\"<PAD>\"]\n",
    "    \n",
    "    #max_len = max(map(lambda x : len(x), sequences))\n",
    "    max_seq_len = max(map(lambda x : len(x), sequences))\n",
    "    max_word_len = max(map(lambda char_nest_list: max(map(lambda char_list: len(char_list), char_nest_list)), sequences))\n",
    "    #print(max_seq_len, max_word_len)\n",
    "    \n",
    "    #seq_list, seq_len_list = [], []\n",
    "    #### [B, S, W], [B,], [B, S]\n",
    "    char_list, seq_len_list, word_len_nest_list = [], [], []\n",
    "    for seq in sequences:\n",
    "        #### [...]\n",
    "        seq = list(seq)\n",
    "        \n",
    "        char_list.append([])\n",
    "        word_len_nest_list.append([])\n",
    "        for word in seq:\n",
    "            word_len_nest_list[-1].append(min(len(word), max_word_len))\n",
    "            word_ = word[:max_word_len] + [pad_mark] * max(max_word_len - len(word), 0)\n",
    "            char_list[-1].append(word_)\n",
    "        \n",
    "        seq_len_list.append(len(char_list[-1]))\n",
    "        for _ in range(max(max_seq_len - len(seq), 0)):\n",
    "            char_list[-1].append([pad_mark] * max_word_len)\n",
    "            word_len_nest_list[-1].append(0)\n",
    "        #seq_ = seq[:max_len] + [pad_mark] * max(max_len - len(seq), 0)\n",
    "        #seq_list.append(seq_)\n",
    "        #seq_len_list.append(min(len(seq), max_len))\n",
    "    #return seq_list, seq_len_list\n",
    "    return char_list, seq_len_list, word_len_nest_list\n",
    "\n",
    "def char2id_build(word2id_pkl_path):\n",
    "    from functools import reduce\n",
    "    with open(word2id_pkl_path, \"rb\") as f:\n",
    "        word2id_dict = pkl.load(f)\n",
    "    char2id_dict = dict(map(lambda t2: (t2[1], t2[0]), enumerate(list(reduce(lambda a, b: a.union(b) ,map(lambda word: set(list(word)),word2id_dict.keys()))))))\n",
    "    padding_idx = len(char2id_dict)\n",
    "    empty_idx = len(char2id_dict) + 1\n",
    "    char2id_dict[\"<PAD>\"] = padding_idx\n",
    "    char2id_dict[\"<UNK>\"] = empty_idx\n",
    "    assert len(char2id_dict) == max(char2id_dict.values()) + 1\n",
    "    assert min(char2id_dict.values()) == 0\n",
    "    print(\"char size {}\".format(len(char2id_dict)))\n",
    "    return char2id_dict\n",
    "\n",
    "multi_col_train_data = read_corpus_by_pd(\"BERT-NER/data/train.txt\")\n",
    "feature_0_to_label = produce_tag2label(multi_col_train_data[1])\n",
    "feature_1_to_label = produce_tag2label(multi_col_train_data[2])\n",
    "tag2label = produce_tag2label(multi_col_train_data[3])\n",
    "\n",
    "def batch_yield_multi(data, batch_size, vocab, feature_0_to_label, feature_1_to_label, tag2label, shuffle=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param data:\n",
    "    :param batch_size:\n",
    "    :param vocab:\n",
    "    :param tag2label:\n",
    "    :param shuffle:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        random.shuffle(data)\n",
    "\n",
    "    #seqs, labels = [], []\n",
    "    seqs, feature_0, feature_1, labels = [], [], [], []\n",
    "\n",
    "    for (sent_, feat0_, feat1_, tag_) in data:\n",
    "        sent_ = sentence2id(sent_, vocab)\n",
    "        label0_ = [feature_0_to_label[tag] for tag in feat0_]\n",
    "        label1_ = [feature_1_to_label[tag] for tag in feat1_]\n",
    "        label_ = [tag2label[tag] for tag in tag_]\n",
    "\n",
    "        if len(seqs) == batch_size:\n",
    "            #yield seqs, labels\n",
    "            yield seqs, feature_0, feature_1, labels\n",
    "            #seqs, labels = [], []\n",
    "            seqs, feature_0, feature_1, labels = [], [], [], []\n",
    "    \n",
    "        seqs.append(sent_)\n",
    "        feature_0.append(label0_)\n",
    "        feature_1.append(label1_)\n",
    "        labels.append(label_)\n",
    "        \n",
    "    if len(seqs) != 0:\n",
    "        yield seqs, feature_0, feature_1, labels\n",
    "        \n",
    "def batch_yield_multi_with_chars(data, batch_size, vocab, char2id, feature_0_to_label, feature_1_to_label, tag2label, shuffle=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param data:\n",
    "    :param batch_size:\n",
    "    :param vocab:\n",
    "    :param tag2label:\n",
    "    :param shuffle:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        random.shuffle(data)\n",
    "\n",
    "    #seqs, labels = [], []\n",
    "    #seqs, feature_0, feature_1, labels = [], [], [], []\n",
    "    chars ,seqs, feature_0, feature_1, labels = [], [], [], [], []\n",
    "\n",
    "    for (sent_, feat0_, feat1_, tag_) in data:\n",
    "        char_ = list(map(lambda word: words_as_char2id(word, char2id), sent_))\n",
    "        \n",
    "        sent_ = sentence2id(sent_, vocab)\n",
    "        label0_ = [feature_0_to_label[tag] for tag in feat0_]\n",
    "        label1_ = [feature_1_to_label[tag] for tag in feat1_]\n",
    "        label_ = [tag2label[tag] for tag in tag_]\n",
    "\n",
    "        if len(seqs) == batch_size:\n",
    "            #yield seqs, labels\n",
    "            #yield seqs, feature_0, feature_1, labels\n",
    "            yield  chars, seqs, feature_0, feature_1, labels\n",
    "            #seqs, labels = [], []\n",
    "            #seqs, feature_0, feature_1, labels = [], [], [], []\n",
    "            chars ,seqs, feature_0, feature_1, labels = [], [], [], [], []\n",
    "        \n",
    "        chars.append(char_)\n",
    "        \n",
    "        seqs.append(sent_)\n",
    "        feature_0.append(label0_)\n",
    "        feature_1.append(label1_)\n",
    "        labels.append(label_)\n",
    "        \n",
    "    if len(seqs) != 0:\n",
    "        #yield seqs, feature_0, feature_1, labels\n",
    "        yield  chars, seqs, feature_0, feature_1, labels\n",
    "        \n",
    "import logging, sys, argparse\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    # copy from StackOverflow\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n",
    "def get_entity(tag_seq, char_seq):\n",
    "    PER = get_PER_entity(tag_seq, char_seq)\n",
    "    LOC = get_LOC_entity(tag_seq, char_seq)\n",
    "    ORG = get_ORG_entity(tag_seq, char_seq)\n",
    "    return PER, LOC, ORG\n",
    "\n",
    "\n",
    "def get_PER_entity(tag_seq, char_seq):\n",
    "    length = len(char_seq)\n",
    "    PER = []\n",
    "    for i, (char, tag) in enumerate(zip(char_seq, tag_seq)):\n",
    "        if tag == 'B-PER':\n",
    "            if 'per' in locals().keys():\n",
    "                PER.append(per)\n",
    "                del per\n",
    "            per = char\n",
    "            if i+1 == length:\n",
    "                PER.append(per)\n",
    "        if tag == 'I-PER':\n",
    "            per += char\n",
    "            if i+1 == length:\n",
    "                PER.append(per)\n",
    "        if tag not in ['I-PER', 'B-PER']:\n",
    "            if 'per' in locals().keys():\n",
    "                PER.append(per)\n",
    "                del per\n",
    "            continue\n",
    "    return PER\n",
    "\n",
    "\n",
    "def get_LOC_entity(tag_seq, char_seq):\n",
    "    length = len(char_seq)\n",
    "    LOC = []\n",
    "    for i, (char, tag) in enumerate(zip(char_seq, tag_seq)):\n",
    "        if tag == 'B-LOC':\n",
    "            if 'loc' in locals().keys():\n",
    "                LOC.append(loc)\n",
    "                del loc\n",
    "            loc = char\n",
    "            if i+1 == length:\n",
    "                LOC.append(loc)\n",
    "        if tag == 'I-LOC':\n",
    "            loc += char\n",
    "            if i+1 == length:\n",
    "                LOC.append(loc)\n",
    "        if tag not in ['I-LOC', 'B-LOC']:\n",
    "            if 'loc' in locals().keys():\n",
    "                LOC.append(loc)\n",
    "                del loc\n",
    "            continue\n",
    "    return LOC\n",
    "\n",
    "\n",
    "def get_ORG_entity(tag_seq, char_seq):\n",
    "    length = len(char_seq)\n",
    "    ORG = []\n",
    "    for i, (char, tag) in enumerate(zip(char_seq, tag_seq)):\n",
    "        if tag == 'B-ORG':\n",
    "            if 'org' in locals().keys():\n",
    "                ORG.append(org)\n",
    "                del org\n",
    "            org = char\n",
    "            if i+1 == length:\n",
    "                ORG.append(org)\n",
    "        if tag == 'I-ORG':\n",
    "            org += char\n",
    "            if i+1 == length:\n",
    "                ORG.append(org)\n",
    "        if tag not in ['I-ORG', 'B-ORG']:\n",
    "            if 'org' in locals().keys():\n",
    "                ORG.append(org)\n",
    "                del org\n",
    "            continue\n",
    "    return ORG\n",
    "\n",
    "\n",
    "def get_logger(filename):\n",
    "    logger = logging.getLogger('logger')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n",
    "    handler = logging.FileHandler(filename)\n",
    "    handler.setLevel(logging.DEBUG)\n",
    "    handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
    "    logging.getLogger().addHandler(handler)\n",
    "    return logger\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22927\n",
      "vocab_size: 22927\n",
      "char size 87\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, argparse, time, random\n",
    "#from model import BiLSTM_CRF\n",
    "#from utils import str2bool, get_logger, get_entity\n",
    "#from data import read_corpus, read_dictionary, tag2label, random_embedding\n",
    "\n",
    "\n",
    "## Session configuration\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # default: 0\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2  # need ~700MB GPU memory\n",
    "\n",
    "args = params_tuple\n",
    "args.mode = \"train\"\n",
    "args.mode\n",
    "\n",
    "vocab_path = \"multi_data_path/word2id.pkl\"\n",
    "import shutil\n",
    "if os.path.exists(vocab_path):\n",
    "    os.remove(vocab_path)\n",
    "    #shutil.rmtree(vocab_path)\n",
    "if not os.path.exists(vocab_path.split(\"/\")[0]):\n",
    "    os.mkdir(vocab_path.split(\"/\")[0])\n",
    "\n",
    "#corpus_path = \"/home/svjack/temp_dir/BERT-NER/data/train.txt\"\n",
    "corpus_path = \"BERT-NER/data/train.txt\"\n",
    "min_count = 0\n",
    "vocab_build(vocab_path=vocab_path, corpus_path=corpus_path, min_count=min_count)\n",
    "\n",
    "train_path = corpus_path\n",
    "#test_path = \"/home/svjack/temp_dir/BERT-NER/data/test.txt\"\n",
    "test_path = \"BERT-NER/data/test.txt\"\n",
    "word2id_pkl_path = \"multi_data_path/word2id.pkl\"\n",
    "word2id = read_dictionary(word2id_pkl_path)\n",
    "\n",
    "char2id = char2id_build(word2id_pkl_path)\n",
    "args.train_data = \"multi_data_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<class '__main__.params_tuple'>\n"
     ]
    }
   ],
   "source": [
    "args.char_embedding_dim = 100\n",
    "if args.pretrain_embedding == 'random':\n",
    "    embeddings = random_embedding(word2id, args.embedding_dim)\n",
    "else:\n",
    "    embedding_path = 'pretrain_embedding.npy'\n",
    "    embeddings = np.array(np.load(embedding_path), dtype='float32')\n",
    "char_embeddings = random_embedding(char2id, args.char_embedding_dim)\n",
    "\n",
    "## read corpus and get training data\n",
    "if args.mode != 'demo':\n",
    "    #train_path = os.path.join('.', args.train_data, 'train_data')\n",
    "    #test_path = os.path.join('.', args.test_data, 'test_data')\n",
    "    #train_data = read_corpus(train_path)\n",
    "    #test_data = read_corpus(test_path); test_size = len(test_data)\n",
    "    #train_data = read_corpus(train_path)\n",
    "    train_data = list(read_corpus_by_pd_nest_list(train_path))\n",
    "    #test_data = read_corpus(test_path); test_size = len(test_data)\n",
    "    test_data = list(read_corpus_by_pd_nest_list(test_path)); test_size = len(test_data)\n",
    "\n",
    "## paths setting\n",
    "paths = {}\n",
    "timestamp = str(int(time.time())) if args.mode == 'train' else args.demo_model\n",
    "output_path = os.path.join('.', args.train_data+\"_save\", timestamp)\n",
    "if not os.path.exists(output_path): os.makedirs(output_path)\n",
    "summary_path = os.path.join(output_path, \"summaries\")\n",
    "paths['summary_path'] = summary_path\n",
    "if not os.path.exists(summary_path): os.makedirs(summary_path)\n",
    "model_path = os.path.join(output_path, \"checkpoints/\")\n",
    "if not os.path.exists(model_path): os.makedirs(model_path)\n",
    "ckpt_prefix = os.path.join(model_path, \"model\")\n",
    "paths['model_path'] = ckpt_prefix\n",
    "result_path = os.path.join(output_path, \"results\")\n",
    "paths['result_path'] = result_path\n",
    "if not os.path.exists(result_path): os.makedirs(result_path)\n",
    "log_path = os.path.join(result_path, \"log.txt\")\n",
    "paths['log_path'] = log_path\n",
    "get_logger(log_path).info(str(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary_path': './multi_data_path_save/1597451905/summaries',\n",
       " 'model_path': './multi_data_path_save/1597451905/checkpoints/model',\n",
       " 'result_path': './multi_data_path_save/1597451905/results',\n",
       " 'log_path': './multi_data_path_save/1597451905/results/log.txt'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /home/svjack/temp_dir/zh-NER-TF/*.pl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def conlleval(label_predict, label_path, metric_path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param label_predict:\n",
    "    :param label_path:\n",
    "    :param metric_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    eval_perl = \"./conlleval_rev.pl\"\n",
    "    with open(label_path, \"w\") as fw:\n",
    "        line = []\n",
    "        for sent_result in label_predict:\n",
    "            for char, tag, tag_ in sent_result:\n",
    "                tag = '0' if tag == 'O' else tag\n",
    "                char = char.encode(\"utf-8\")\n",
    "                line.append(\"{} {} {}\\n\".format(char, tag, tag_))\n",
    "            line.append(\"\\n\")\n",
    "        fw.writelines(line)\n",
    "    os.system(\"perl {} < {} > {}\".format(eval_perl, label_path, metric_path))\n",
    "    with open(metric_path) as fr:\n",
    "        metrics = [line.strip() for line in fr]\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, time, sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell\n",
    "from tensorflow.contrib.crf import crf_log_likelihood\n",
    "from tensorflow.contrib.crf import viterbi_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF_MULTI_CHAR(object):\n",
    "    def __init__(self, args, embeddings, char_embeddings, feature_0_to_label, feature_1_to_label, tag2label, vocab, char2id, paths, config):\n",
    "        #self.max_char_capacity = 20\n",
    "        self.char2id = char2id\n",
    "        \n",
    "        self.batch_size = args.batch_size\n",
    "        self.epoch_num = args.epoch\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "        self.char_embeddings = char_embeddings\n",
    "        \n",
    "        self.filters_list = args.filters_list\n",
    "        self.kernel_list = args.kernel_list\n",
    "        assert len(self.filters_list) == len(self.kernel_list)\n",
    "        \n",
    "        self.CRF = args.CRF\n",
    "        self.update_embedding = args.update_embedding\n",
    "        self.dropout_keep_prob = args.dropout\n",
    "        self.optimizer = args.optimizer\n",
    "        self.lr = args.lr\n",
    "        self.clip_grad = args.clip\n",
    "        self.tag2label = tag2label\n",
    "        self.num_tags = len(tag2label)\n",
    "        self.vocab = vocab\n",
    "        self.shuffle = args.shuffle\n",
    "        self.model_path = paths['model_path']\n",
    "        self.summary_path = paths['summary_path']\n",
    "        self.logger = get_logger(paths['log_path'])\n",
    "        self.result_path = paths['result_path']\n",
    "        self.config = config\n",
    "        \n",
    "        self.feature_0_to_label = feature_0_to_label\n",
    "        self.feature_1_to_label = feature_1_to_label\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self.add_placeholders()\n",
    "        self.lookup_layer_op()\n",
    "        self.biLSTM_layer_op()\n",
    "        self.softmax_pred_op()\n",
    "        self.loss_op()\n",
    "        self.trainstep_op()\n",
    "        self.init_op()\n",
    "    \n",
    "    def add_placeholders(self):\n",
    "        self.char_ids = tf.placeholder(tf.int32, shape = [None, None, None], name = \"char_ids\")\n",
    "        \n",
    "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None], name=\"word_ids\")\n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, None], name=\"labels\")\n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None], name=\"sequence_lengths\")\n",
    "\n",
    "        self.dropout_pl = tf.placeholder(dtype=tf.float32, shape=[], name=\"dropout\")\n",
    "        self.lr_pl = tf.placeholder(dtype=tf.float32, shape=[], name=\"lr\")\n",
    "        \n",
    "        self.feature0 = tf.placeholder(tf.int32, shape=[None, None], name=\"feature0\")\n",
    "        self.feature1 = tf.placeholder(tf.int32, shape=[None, None], name=\"feature1\")\n",
    "\n",
    "    def lookup_layer_op(self):\n",
    "        with tf.variable_scope(\"words\"):\n",
    "            _word_embeddings = tf.Variable(self.embeddings,\n",
    "                                           dtype=tf.float32,\n",
    "                                           trainable=self.update_embedding,\n",
    "                                           name=\"_word_embeddings\")\n",
    "            word_embeddings = tf.nn.embedding_lookup(params=_word_embeddings,\n",
    "                                                     ids=self.word_ids,\n",
    "                                                     name=\"word_embeddings\")\n",
    "            self.word_embeddings =  tf.nn.dropout(word_embeddings, self.dropout_pl)\n",
    "        \n",
    "        with tf.variable_scope(\"chars\"):\n",
    "            '''\n",
    "            _char_embeddings = tf.Variable(self.embeddings,\n",
    "                                           dtype=tf.float32,\n",
    "                                           trainable=self.update_embedding,\n",
    "                                           name=\"_char_embeddings\")\n",
    "            '''\n",
    "            #char_embeddings\n",
    "            _char_embeddings = tf.Variable(self.char_embeddings,\n",
    "                                           dtype=tf.float32,\n",
    "                                           trainable=self.update_embedding,\n",
    "                                           name=\"_char_embeddings\")\n",
    "            char_embeddings = tf.nn.embedding_lookup(params=_char_embeddings,\n",
    "                                                     ids=self.char_ids,\n",
    "                                                     name=\"char_embeddings\")\n",
    "            #### [B, S, W, dim]\n",
    "            self.char_embeddings =  tf.nn.dropout(char_embeddings, self.dropout_pl)\n",
    "        \n",
    "    def produce_char_output(self, char_embeddings, filters, kernel_size, var_scope = None):\n",
    "        assert type(var_scope) == type(\"\")\n",
    "        with tf.variable_scope(var_scope):\n",
    "            conv2d_layer = tf.layers.conv2d(char_embeddings, kernel_size=kernel_size, filters=filters, padding=\"same\", name = \"char_conv\")      \n",
    "    \n",
    "            conv2d_layer_t = tf.transpose(conv2d_layer, [0, 2, 3, 1])\n",
    "            max2d_layer = tf.layers.MaxPooling2D(pool_size=3, strides=2, padding = \"same\", name = \"char_max\")(conv2d_layer_t)\n",
    "            max2d_layer_t = tf.transpose(max2d_layer, [0, 3, 1, 2])\n",
    "            char_output_max = tf.reduce_max(tf.reshape(max2d_layer_t, [tf.shape(max2d_layer_t)[0], tf.shape(max2d_layer_t)[1], -1]), axis = -1, name = \"char_reduce_max\")\n",
    "            char_expand_max = tf.expand_dims(char_output_max, -1)\n",
    "            \n",
    "            char_output_min = tf.reduce_min(tf.reshape(max2d_layer_t, [tf.shape(max2d_layer_t)[0], tf.shape(max2d_layer_t)[1], -1]), axis = -1, name = \"char_reduce_min\")\n",
    "            char_expand_min = tf.expand_dims(char_output_min, -1)\n",
    "            \n",
    "            char_output_mean = tf.reduce_max(tf.reshape(max2d_layer_t, [tf.shape(max2d_layer_t)[0], tf.shape(max2d_layer_t)[1], -1]), axis = -1, name = \"char_reduce_mean\")\n",
    "            char_expand_mean = tf.expand_dims(char_output_mean, -1)\n",
    "            \n",
    "            return tf.concat([char_expand_max, char_expand_min, char_expand_mean], axis = -1)\n",
    "        \n",
    "    def biLSTM_layer_op(self):\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            cell_fw = LSTMCell(self.hidden_dim)\n",
    "            cell_bw = LSTMCell(self.hidden_dim)\n",
    "            #### word-embed [B, L, N]\n",
    "            #### feat_0 feat_1 [B, L]\n",
    "            \n",
    "            '''\n",
    "            filters = 3\n",
    "            kernel_size = 3\n",
    "            #fake_char_embedding = tf.convert_to_tensor(np.random.random([B, S, W, dim]))\n",
    "            \n",
    "            conv2d_layer = tf.layers.conv2d(self.char_embeddings, kernel_size=kernel_size, filters=filters, padding=\"same\", name = \"char_conv\")      \n",
    "    \n",
    "            conv2d_layer_t = tf.transpose(conv2d_layer, [0, 2, 3, 1])\n",
    "            max2d_layer = tf.layers.MaxPooling2D(pool_size=3, strides=2, padding = \"same\", name = \"char_max\")(conv2d_layer_t)\n",
    "            max2d_layer_t = tf.transpose(max2d_layer, [0, 3, 1, 2])\n",
    "            char_output = tf.reduce_max(tf.reshape(max2d_layer_t, [tf.shape(max2d_layer_t)[0], tf.shape(max2d_layer_t)[1], -1]), axis = -1, name = \"char_reduce_max\")\n",
    "            char_expand = tf.expand_dims(char_output, -1)\n",
    "            '''\n",
    "            char_output_list = []\n",
    "            for iidx, var_scope_name in map(lambda idx: (idx ,\"char_output_{}\".format(idx)), range(len(args.filters_list))):\n",
    "                #### char_embeddings, filters, kernel_size, var_scope\n",
    "                filters, kernel_size = self.filters_list[iidx], self.kernel_list[iidx]\n",
    "                char_output_list.append(self.produce_char_output(self.char_embeddings ,filters, kernel_size, var_scope = var_scope_name))\n",
    "            char_expand = tf.concat(char_output_list, axis = -1)\n",
    "\n",
    "            feat0_expand = tf.expand_dims(self.feature0, -1)\n",
    "            feat0_expand = tf.cast(feat0_expand, tf.float32)\n",
    "            feat1_expand = tf.expand_dims(self.feature1, -1)\n",
    "            feat1_expand = tf.cast(feat1_expand, tf.float32)\n",
    "            #### [B, S, concat-d]\n",
    "            #inputs = tf.concat([self.word_embeddings, feat0_expand, feat1_expand], axis = -1, name = \"bind_inputs\")\n",
    "            inputs = tf.concat([char_expand ,self.word_embeddings, feat0_expand, feat1_expand], axis = -1, name = \"bind_inputs\")\n",
    "            \n",
    "            '''\n",
    "            (output_fw_seq, output_bw_seq), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw=cell_fw,\n",
    "                cell_bw=cell_bw,\n",
    "                inputs=self.word_embeddings,\n",
    "                sequence_length=self.sequence_lengths,\n",
    "                dtype=tf.float32)\n",
    "            '''\n",
    "            (output_fw_seq, output_bw_seq), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw=cell_fw,\n",
    "                cell_bw=cell_bw,\n",
    "                inputs=inputs,\n",
    "                sequence_length=self.sequence_lengths,\n",
    "                dtype=tf.float32)\n",
    "            \n",
    "            output = tf.concat([output_fw_seq, output_bw_seq], axis=-1)\n",
    "            output = tf.nn.dropout(output, self.dropout_pl)\n",
    "\n",
    "        with tf.variable_scope(\"proj\"):\n",
    "            W = tf.get_variable(name=\"W\",\n",
    "                                shape=[2 * self.hidden_dim, self.num_tags],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                dtype=tf.float32)\n",
    "\n",
    "            b = tf.get_variable(name=\"b\",\n",
    "                                shape=[self.num_tags],\n",
    "                                initializer=tf.zeros_initializer(),\n",
    "                                dtype=tf.float32)\n",
    "\n",
    "            s = tf.shape(output)\n",
    "            output = tf.reshape(output, [-1, 2*self.hidden_dim])\n",
    "            pred = tf.matmul(output, W) + b\n",
    "\n",
    "            self.logits = tf.reshape(pred, [-1, s[1], self.num_tags])\n",
    "            \n",
    "    def loss_op(self):\n",
    "        if self.CRF:\n",
    "            log_likelihood, self.transition_params = crf_log_likelihood(inputs=self.logits,\n",
    "                                                                   tag_indices=self.labels,\n",
    "                                                                   sequence_lengths=self.sequence_lengths)\n",
    "            self.loss = -tf.reduce_mean(log_likelihood)\n",
    "\n",
    "        else:\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                    labels=self.labels)\n",
    "            mask = tf.sequence_mask(self.sequence_lengths)\n",
    "            losses = tf.boolean_mask(losses, mask)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "\n",
    "    def softmax_pred_op(self):\n",
    "        if not self.CRF:\n",
    "            self.labels_softmax_ = tf.argmax(self.logits, axis=-1)\n",
    "            self.labels_softmax_ = tf.cast(self.labels_softmax_, tf.int32)\n",
    "    \n",
    "    def trainstep_op(self):\n",
    "        with tf.variable_scope(\"train_step\"):\n",
    "            self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            if self.optimizer == 'Adam':\n",
    "                optim = tf.train.AdamOptimizer(learning_rate=self.lr_pl)\n",
    "            elif self.optimizer == 'Adadelta':\n",
    "                optim = tf.train.AdadeltaOptimizer(learning_rate=self.lr_pl)\n",
    "            elif self.optimizer == 'Adagrad':\n",
    "                optim = tf.train.AdagradOptimizer(learning_rate=self.lr_pl)\n",
    "            elif self.optimizer == 'RMSProp':\n",
    "                optim = tf.train.RMSPropOptimizer(learning_rate=self.lr_pl)\n",
    "            elif self.optimizer == 'Momentum':\n",
    "                optim = tf.train.MomentumOptimizer(learning_rate=self.lr_pl, momentum=0.9)\n",
    "            elif self.optimizer == 'SGD':\n",
    "                optim = tf.train.GradientDescentOptimizer(learning_rate=self.lr_pl)\n",
    "            else:\n",
    "                optim = tf.train.GradientDescentOptimizer(learning_rate=self.lr_pl)\n",
    "\n",
    "            grads_and_vars = optim.compute_gradients(self.loss)\n",
    "            \n",
    "            grads_and_vars_clip = [[tf.clip_by_value(g, -self.clip_grad, self.clip_grad), v] for g, v in grads_and_vars]\n",
    "            self.train_op = optim.apply_gradients(grads_and_vars_clip, global_step=self.global_step)\n",
    "\n",
    "    def init_op(self):\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def add_summary(self, sess):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sess:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        self.file_writer = tf.summary.FileWriter(self.summary_path, sess.graph)\n",
    "            \n",
    "    def train(self, train, dev):\n",
    "        \"\"\"\n",
    "\n",
    "        :param train:\n",
    "        :param dev:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "        with tf.Session(config=self.config) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            self.add_summary(sess)\n",
    "\n",
    "            for epoch in range(self.epoch_num):\n",
    "                self.run_one_epoch(sess, train, dev, self.tag2label, epoch, saver)\n",
    "\n",
    "    def test(self, test):\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session(config=self.config) as sess:\n",
    "            self.logger.info('=========== testing ===========')\n",
    "            saver.restore(sess, self.model_path)\n",
    "            label_list, seq_len_list = self.dev_one_epoch(sess, test)\n",
    "            self.evaluate(label_list, seq_len_list, test)\n",
    "    \n",
    "    def get_feed_dict(self, chars ,seqs, feat0, feat1, labels=None, lr=None, dropout=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param seqs:\n",
    "        :param labels:\n",
    "        :param lr:\n",
    "        :param dropout:\n",
    "        :return: feed_dict\n",
    "        \"\"\"\n",
    "        char_ids, seq_len_list, word_len_nest_list = pad_char_sequences(chars, self.char2id)\n",
    "        \n",
    "        word_ids, seq_len_list = pad_sequences(seqs, pad_mark=0)\n",
    "        feat0_, _ = pad_sequences(feat0, pad_mark=-1)\n",
    "        feat1_, _ = pad_sequences(feat1, pad_mark=-1)\n",
    "        \n",
    "        #feed_dict = {self.word_ids: word_ids,\n",
    "        #             self.sequence_lengths: seq_len_list}\n",
    "        '''\n",
    "        feed_dict = {self.word_ids: word_ids,\n",
    "                     self.sequence_lengths: seq_len_list,\n",
    "                    self.feature0: feat0_,\n",
    "                     self.feature1: feat1_,\n",
    "                    }\n",
    "        '''\n",
    "        #print(np.asarray(char_ids).shape)\n",
    "        #print(\"-\"*100)\n",
    "        \n",
    "        feed_dict = {\n",
    "            self.char_ids: char_ids,\n",
    "            self.word_ids: word_ids,\n",
    "                     self.sequence_lengths: seq_len_list,\n",
    "                    self.feature0: feat0_,\n",
    "                     self.feature1: feat1_,\n",
    "                    }\n",
    "        \n",
    "        if labels is not None:\n",
    "            labels_, _ = pad_sequences(labels, pad_mark=0)\n",
    "            feed_dict[self.labels] = labels_\n",
    "        if lr is not None:\n",
    "            feed_dict[self.lr_pl] = lr\n",
    "        if dropout is not None:\n",
    "            feed_dict[self.dropout_pl] = dropout\n",
    "\n",
    "        return feed_dict, seq_len_list\n",
    "    \n",
    "    #def predict_one_batch(self, sess, seqs, feat0, feat1):\n",
    "    def predict_one_batch(self, sess, chars, seqs, feat0, feat1):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sess:\n",
    "        :param seqs:\n",
    "        :return: label_list\n",
    "                 seq_len_list\n",
    "        \"\"\"\n",
    "        #feed_dict, seq_len_list = self.get_feed_dict(seqs, feat0, feat1, dropout=1.0)\n",
    "        feed_dict, seq_len_list = self.get_feed_dict(chars ,seqs, feat0, feat1, dropout=1.0)\n",
    "        \n",
    "        if self.CRF:\n",
    "            logits, transition_params = sess.run([self.logits, self.transition_params],\n",
    "                                                 feed_dict=feed_dict)\n",
    "            label_list = []\n",
    "            for logit, seq_len in zip(logits, seq_len_list):\n",
    "                viterbi_seq, _ = viterbi_decode(logit[:seq_len], transition_params)\n",
    "                label_list.append(viterbi_seq)\n",
    "            return label_list, seq_len_list\n",
    "\n",
    "        else:\n",
    "            label_list = sess.run(self.labels_softmax_, feed_dict=feed_dict)\n",
    "            return label_list, seq_len_list\n",
    "    \n",
    "    def demo_one(self, sess, sent):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sess:\n",
    "        :param sent: \n",
    "        :return:\n",
    "        \"\"\"\n",
    "        label_list = []\n",
    "        '''\n",
    "        for seqs, labels in batch_yield(sent, self.batch_size, self.vocab, self.tag2label, shuffle=False):\n",
    "        '''\n",
    "        #for seqs, feat0, feat1, labels in batch_yield_multi(sent, self.batch_size, self.vocab, self.feature_0_to_label, self.feature_1_to_label, self.tag2label, shuffle=False):\n",
    "        for chars ,seqs, feat0, feat1, labels in batch_yield_multi_with_chars(sent, self.batch_size, self.vocab, self.char2id, self.feature_0_to_label, self.feature_1_to_label, self.tag2label, shuffle=False):\n",
    "                    \n",
    "            label_list_, _ = self.predict_one_batch(sess, chars, seqs, feat0, feat1)\n",
    "            label_list.extend(label_list_)\n",
    "        label2tag = {}\n",
    "        for tag, label in self.tag2label.items():\n",
    "            label2tag[label] = tag if label != 0 else label\n",
    "        tag = [label2tag[label] for label in label_list[0]]\n",
    "        return tag\n",
    "    \n",
    "    def run_one_epoch(self, sess, train, dev, tag2label, epoch, saver):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sess:\n",
    "        :param train:\n",
    "        :param dev:\n",
    "        :param tag2label:\n",
    "        :param epoch:\n",
    "        :param saver:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        num_batches = (len(train) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "        start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "        #batches = batch_yield(train, self.batch_size, self.vocab, self.tag2label, shuffle=self.shuffle)\n",
    "        #batches = batch_yield_multi(train, self.batch_size, self.vocab, self.feature_0_to_label, self.feature_1_to_label, self.tag2label, shuffle=self.shuffle)\n",
    "        batches = batch_yield_multi_with_chars(train, self.batch_size, self.vocab, self.char2id, self.feature_0_to_label, self.feature_1_to_label, self.tag2label, shuffle=self.shuffle)\n",
    "        \n",
    "        #for step, (seqs, labels) in enumerate(batches):\n",
    "        #for step, (seqs, feat0, feat1, labels) in enumerate(batches):\n",
    "        for step, (chars ,seqs, feat0, feat1, labels) in enumerate(batches):\n",
    "            sys.stdout.write(' processing: {} batch / {} batches.'.format(step + 1, num_batches) + '\\r')\n",
    "            step_num = epoch * num_batches + step + 1\n",
    "            '''\n",
    "            feed_dict, _ = self.get_feed_dict(seqs, labels, self.lr, self.dropout_keep_prob)\n",
    "            '''\n",
    "            #feed_dict, _ = self.get_feed_dict(seqs, feat0, feat1, labels, self.lr, self.dropout_keep_prob)\n",
    "            feed_dict, _ = self.get_feed_dict(chars ,seqs, feat0, feat1, labels, self.lr, self.dropout_keep_prob)\n",
    "            _, loss_train, summary, step_num_ = sess.run([self.train_op, self.loss, self.merged, self.global_step],\n",
    "                                                         feed_dict=feed_dict)\n",
    "            #break\n",
    "            if step + 1 == 1 or (step + 1) % 300 == 0 or step + 1 == num_batches:\n",
    "                self.logger.info(\n",
    "                    '{} epoch {}, step {}, loss: {:.4}, global_step: {}'.format(start_time, epoch + 1, step + 1,\n",
    "                                                                                loss_train, step_num))\n",
    "\n",
    "            self.file_writer.add_summary(summary, step_num)\n",
    "            \n",
    "            #print((step + 1, num_batches))\n",
    "            if step + 1 == num_batches:\n",
    "                saver.save(sess, self.model_path, global_step=step_num)\n",
    "        \n",
    "        saver.save(sess, self.model_path, global_step=step_num)\n",
    "        self.logger.info('===========validation / test===========')\n",
    "        label_list_dev, seq_len_list_dev = self.dev_one_epoch(sess, dev)\n",
    "        self.evaluate(label_list_dev, seq_len_list_dev, dev, epoch)\n",
    "\n",
    "    def dev_one_epoch(self, sess, dev):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sess:\n",
    "        :param dev:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        label_list, seq_len_list = [], []\n",
    "        #for seqs, labels in batch_yield(dev, self.batch_size, self.vocab, self.tag2label, shuffle=False):\n",
    "        #for seqs, feat0, feat1, labels in batch_yield_multi(dev, self.batch_size, self.vocab, self.feature_0_to_label, self.feature_1_to_label, self.tag2label, shuffle=False):\n",
    "        for chars ,seqs, feat0, feat1, labels in batch_yield_multi_with_chars(dev, self.batch_size, self.vocab, self.char2id, self.feature_0_to_label, self.feature_1_to_label, self.tag2label, shuffle=False):\n",
    "\n",
    "            #label_list_, seq_len_list_ = self.predict_one_batch(sess, seqs)\n",
    "            label_list_, seq_len_list_ = self.predict_one_batch(sess, chars, seqs, feat0, feat1)\n",
    "            label_list.extend(label_list_)\n",
    "            seq_len_list.extend(seq_len_list_)\n",
    "        return label_list, seq_len_list\n",
    "\n",
    "    def evaluate(self, label_list, seq_len_list, data, epoch=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param label_list:\n",
    "        :param seq_len_list:\n",
    "        :param data:\n",
    "        :param epoch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        label2tag = {}\n",
    "        for tag, label in self.tag2label.items():\n",
    "            label2tag[label] = tag if label != 0 else label\n",
    "\n",
    "        model_predict = []\n",
    "        #for label_, (sent, tag) in zip(label_list, data):\n",
    "        for label_, (sent, _, _, tag) in zip(label_list, data):\n",
    "            tag_ = [label2tag[label__] for label__ in label_]\n",
    "            sent_res = []\n",
    "            if  len(label_) != len(sent):\n",
    "                print(sent)\n",
    "                print(len(label_))\n",
    "                print(tag)\n",
    "            for i in range(len(sent)):\n",
    "                sent_res.append([sent[i], tag[i], tag_[i]])\n",
    "            model_predict.append(sent_res)\n",
    "        #print(model_predict)\n",
    "        epoch_num = str(epoch+1) if epoch != None else 'test'\n",
    "        label_path = os.path.join(self.result_path, 'label_' + epoch_num)\n",
    "        metric_path = os.path.join(self.result_path, 'result_metric_' + epoch_num)\n",
    "        for _ in conlleval(model_predict, label_path, metric_path):\n",
    "            self.logger.info(_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /home/svjack/temp_dir/zh-NER-TF/multi_data_path_save/1597370783/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-ef96684abe84>:67: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From <ipython-input-12-ef96684abe84>:67: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-ef96684abe84>:108: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From <ipython-input-12-ef96684abe84>:108: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-ef96684abe84>:90: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From <ipython-input-12-ef96684abe84>:90: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-ef96684abe84>:154: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From <ipython-input-12-ef96684abe84>:154: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /home/svjack/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args.filters_list = [3, 5]\n",
    "args.kernel_list = [3, 5]\n",
    "model = BiLSTM_CRF_MULTI_CHAR(args, embeddings, char_embeddings, feature_0_to_label, feature_1_to_label, tag2label, word2id, char2id, paths, config = config)\n",
    "model.build_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 16122\n",
      " processing: 1 batch / 252 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-15 08:43:59 epoch 1, step 1, loss: 29.01, global_step: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 252 batch / 252 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-15 08:43:59 epoch 1, step 252, loss: 1.818, global_step: 252\n",
      "===========validation / test===========\n",
      "processed 50154 tokens with 5648 phrases; found: 5446 phrases; correct: 3863.\n",
      "accuracy:  94.68%; precision:  70.93%; recall:  68.40%; FB1:  69.64\n",
      "LOC: precision:  82.72%; recall:  75.48%; FB1:  78.93  1522\n",
      "MISC: precision:  69.32%; recall:  53.42%; FB1:  60.34  541\n",
      "ORG: precision:  59.94%; recall:  62.25%; FB1:  61.08  1725\n",
      "PER: precision:  72.07%; recall:  73.90%; FB1:  72.98  1658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 252 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-15 08:49:34 epoch 2, step 1, loss: 1.968, global_step: 253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 252 batch / 252 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-15 08:49:34 epoch 2, step 252, loss: 0.8452, global_step: 504\n",
      "===========validation / test===========\n",
      "processed 50154 tokens with 5648 phrases; found: 5664 phrases; correct: 4475.\n",
      "accuracy:  96.37%; precision:  79.01%; recall:  79.23%; FB1:  79.12\n",
      "LOC: precision:  91.26%; recall:  80.10%; FB1:  85.31  1464\n",
      "MISC: precision:  68.04%; recall:  66.10%; FB1:  67.05  682\n",
      "ORG: precision:  75.98%; recall:  74.47%; FB1:  75.22  1628\n",
      "PER: precision:  76.08%; recall:  88.93%; FB1:  82.01  1890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 252 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-15 08:55:12 epoch 3, step 1, loss: 0.9204, global_step: 505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 252 batch / 252 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-15 08:55:12 epoch 3, step 252, loss: 0.4852, global_step: 756\n",
      "===========validation / test===========\n",
      "processed 50154 tokens with 5648 phrases; found: 5637 phrases; correct: 4552.\n",
      "accuracy:  96.58%; precision:  80.75%; recall:  80.59%; FB1:  80.67\n",
      "LOC: precision:  89.76%; recall:  83.57%; FB1:  86.56  1553\n",
      "MISC: precision:  70.48%; recall:  69.37%; FB1:  69.92  691\n",
      "ORG: precision:  81.61%; recall:  72.43%; FB1:  76.75  1474\n",
      "PER: precision:  76.50%; recall:  90.79%; FB1:  83.03  1919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 252 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-15 09:00:59 epoch 4, step 1, loss: 0.4359, global_step: 757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 252 batch / 252 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-15 09:00:59 epoch 4, step 252, loss: 0.2464, global_step: 1008\n",
      "===========validation / test===========\n",
      "processed 50154 tokens with 5648 phrases; found: 5630 phrases; correct: 4593.\n",
      "accuracy:  96.72%; precision:  81.58%; recall:  81.32%; FB1:  81.45\n",
      "LOC: precision:  93.05%; recall:  83.51%; FB1:  88.03  1497\n",
      "MISC: precision:  69.91%; recall:  69.52%; FB1:  69.71  698\n",
      "ORG: precision:  81.87%; recall:  75.02%; FB1:  78.29  1522\n",
      "PER: precision:  76.63%; recall:  90.66%; FB1:  83.06  1913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 252 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-15 09:05:44 epoch 5, step 1, loss: 0.2535, global_step: 1009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 84 batch / 252 batches.\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-30596d7f88b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train data: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-ef96684abe84>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train, dev)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag2label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ef96684abe84>\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(self, sess, train, dev, tag2label, epoch, saver)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             _, loss_train, summary, step_num_ = sess.run([self.train_op, self.loss, self.merged, self.global_step],\n\u001b[0;32m--> 377\u001b[0;31m                                                          feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;31m#break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m300\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"train data: {}\".format(len(train_data)))\n",
    "model.train(train=train_data, dev=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./multi_data_path_save/1597451905/checkpoints/model'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "model_dict[\"tag2label\"] , model_dict[\"feature_0_to_label\"] , model_dict[\"feature_1_to_label\"] \\\n",
    ", model_dict[\"vocab\"] , model_dict[\"char2id\"] = \\\n",
    "model.tag2label, model.feature_0_to_label, model.feature_1_to_label, \\\n",
    "model.vocab, model.char2id\n",
    "model_dict_path = \"model_dict\"\n",
    "if os.path.exists(model_dict_path):\n",
    "    os.remove(model_dict_path)\n",
    "with open(model_dict_path, \"wb\") as f:\n",
    "    pkl.dump(model_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths['model_path'] = \"/home/svjack/temp_dir/colab-model/multi_data_path_save/1597451905/checkpoints/model-1008\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.filters_list = [3, 5]\n",
    "args.kernel_list = [3, 5]\n",
    "model = BiLSTM_CRF_MULTI_CHAR(args, embeddings, char_embeddings, feature_0_to_label, feature_1_to_label, tag2label, word2id, char2id, paths, config = config)\n",
    "model.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=========== testing ===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/svjack/temp_dir/colab-model/multi_data_path_save/1597451905/checkpoints/model-1008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from /home/svjack/temp_dir/colab-model/multi_data_path_save/1597451905/checkpoints/model-1008\n",
      "processed 50154 tokens with 5648 phrases; found: 5630 phrases; correct: 4593.\n",
      "accuracy:  96.72%; precision:  81.58%; recall:  81.32%; FB1:  81.45\n",
      "LOC: precision:  93.05%; recall:  83.51%; FB1:  88.03  1497\n",
      "MISC: precision:  69.91%; recall:  69.52%; FB1:  69.71  698\n",
      "ORG: precision:  81.87%; recall:  75.02%; FB1:  78.29  1522\n",
      "PER: precision:  76.63%; recall:  90.66%; FB1:  83.06  1913\n"
     ]
    }
   ],
   "source": [
    "model.test(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/svjack/temp_dir/colab-model/multi_data_path_save/1597451905/checkpoints/model-1008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from /home/svjack/temp_dir/colab-model/multi_data_path_save/1597451905/checkpoints/model-1008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 252 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-15 10:10:29 epoch 1, step 1, loss: 0.1953, global_step: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 252 batch / 252 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-15 10:10:29 epoch 1, step 252, loss: 0.656, global_step: 252\n",
      "===========validation / test===========\n",
      "processed 50154 tokens with 5648 phrases; found: 5645 phrases; correct: 4652.\n",
      "accuracy:  96.83%; precision:  82.41%; recall:  82.37%; FB1:  82.39\n",
      "LOC: precision:  89.61%; recall:  86.39%; FB1:  87.97  1608\n",
      "MISC: precision:  69.10%; recall:  72.93%; FB1:  70.96  741\n",
      "ORG: precision:  80.08%; recall:  76.22%; FB1:  78.10  1581\n",
      "PER: precision:  83.56%; recall:  88.62%; FB1:  86.01  1715\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "with tf.Session(config=model.config) as sess:\n",
    "    #sess.run(model.init_op)\n",
    "    saver.restore(sess, model.model_path)\n",
    "    model.add_summary(sess)\n",
    "\n",
    "    for epoch in range(model.epoch_num):\n",
    "        #model.run_one_epoch(sess, train, dev, model.tag2label, epoch, saver)\n",
    "        model.run_one_epoch(sess, train_data, test_data, model.tag2label, epoch, saver)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
